### âœ… Instructions to Create the Data Pipeline

---

#### **Step 1: Navigate to the Project Folder**
Go to the `Assignment/01_data_pipeline/scripts` folder.

---

#### **Step 2: Refactor the Data Cleaning Code**
Break down all data cleaning tasks (from the data cleaning notebook) into functions:

- Refer to the instructions mentioned in the `utils.py` file (located in the `scripts` folder).
- Copy the relevant code from `data_cleaning_template.ipynb` (located in `Assignment/01_data_pipeline/notebooks`) and paste it into the corresponding function bodies in `utils.py`.

---

#### **Step 3: Use Constants Instead of Function Arguments**
When defining the functions:

- **Do not** pass input variables as function arguments.
- Use the constants already defined in `constants.py` and `significant_categorical_level.py`.
- Set the appropriate values for them and import them in your functions.

---

#### **Step 4: Store Significant Levels**
For columns `first_platform_c`, `first_utm_medium_c`, and `first_utm_source_c`:

- Map the bottom 10 percentile values to `'others'`.
- Store the list of **significant levels** for these columns in `significant_categorical_level.py`.
- Import and use these variables in `utils.py` instead of hardcoding the lists.

> ðŸ”¸ *Note: The variables in `significant_categorical_level.py` are currently empty lists. You need to fill them in.*

---

#### **Step 5: Modify Functions to Use a Database**
Once the functions are created:

- Modify them to **read input** and **write output** to a database: `utils_output.db`.
- Follow the function execution order below:
  ```
  build_dbs -> load_data_into_db -> map_city_tier -> map_categorical_vars -> interactions_mapping
  ```
- Follow the **table naming convention** as per the docstrings.
- The DB `utils_output.db` will be created by the `build_dbs` function in the `scripts` folder.

---

#### **Step 6: Create a Debug Notebook**
Create a dummy notebook in the `scripts` folder:

- Import `utils.py` and run all the functions in the correct order.
- Use it to debug and ensure `utils_output.db` is created successfully.

---

#### **Step 7: Centralize Constants**
After validating that the functions work:

- Move all constant values into `constants.py`.
- Re-run the functions from **Step 5** to ensure everything still works.

> ðŸ”¸ *Note: You may create additional variables if needed, but ensure that `utils.py` and `constants.py` always reside in the same directory.*

---

#### **Step 8: Create the Unit Test Folder**
Create a new folder named `unit_test`:

- Copy all files (except the data folder) from `scripts` into `unit_test`.
- Do not delete anything from `scripts`.

The `unit_test` folder must include:

```
â”œâ”€â”€ city_tier_mapping.py
â”œâ”€â”€ constants.py
â”œâ”€â”€ data_validation_checks.py
â”œâ”€â”€ dummy.ipynb
â”œâ”€â”€ interaction_mapping.csv
â”œâ”€â”€ lead_scoring_data_pipeline.py
â”œâ”€â”€ leadscoring_test.csv
â”œâ”€â”€ schema.py
â”œâ”€â”€ significant_categorical_level.py
â”œâ”€â”€ test_with_pytest.py
â”œâ”€â”€ unit_test_cases.db
â”œâ”€â”€ utils.py
â””â”€â”€ utils_output.db
```

> ðŸ”¸ *Note: The input for testing (`load_data_into_db`) is `leadscoring_test.csv`. Outputs of other functions should match expected outputs in `unit_test_cases.db`.*

---

#### **Step 9: Write Unit Tests**
In `unit_test/test_with_pytest.py`, write test cases to compare actual outputs to expected outputs in `unit_test_cases.db`.

Expected output table names:

```
load_data_into_db           -> loaded_data_test_case
map_city_tier               -> city_tier_mapped_test_case
map_categorical_vars        -> categorical_variables_mapped_test_case
interactions_mapping        -> interactions_mapped_test_case
```

> ðŸ”¸ *Make sure to adjust `utils.py` in the `unit_test` folder accordingly. Do NOT change the original file in `scripts`.*

---

#### **Step 10: Organize Mapping Files**
Once tests pass:

- In `scripts`, create a folder named `mapping`.
- Move the following files into it:
  ```
  city_tier_mapping.py
  interaction_mapping.csv
  significant_categorical_level.py
  ```
- Update **import statements** to reflect the new folder structure.

---

#### **Step 11: Data Validation**
Open `data_validation_check.py` and implement functions in this execution order:

```
build_dbs -> raw_data_schema_check -> load_data_into_db -> map_city_tier -> map_categorical_vars -> interactions_mapping -> model_input_schema_check
```

- Read/write from the **correct tables** in the DB.
- Use schema definitions from `schema.py`.

> ðŸ”¸ *No changes should be made to `utils.py` for this step.*

---

#### **Step 12: Validate in Notebook**
Use the same dummy notebook to validate that `data_validation_check.py` functions execute as expected.

---

#### **Step 13: Create Airflow Pipeline**
Open `lead_scoring_data_pipeline.py` and follow the instructions inside to define the Airflow DAG.

---

#### **Step 14: Setup DAG Folder in Airflow**
Go to `~/airflow/dags/` and create a new folder:

```
Lead_scoring_data_pipeline
```

It should contain:

```
â”œâ”€â”€ data
â”‚   â””â”€â”€ data/leadscoring.csv
â”œâ”€â”€ mapping
â”‚   â”œâ”€â”€ mapping/city_tier_mapping.py
â”‚   â”œâ”€â”€ mapping/interaction_mapping.csv
â”‚   â””â”€â”€ mapping/significant_categorical_level.py
â”œâ”€â”€ schema.py
â”œâ”€â”€ data_validation_checks.py
â”œâ”€â”€ lead_scoring_data_pipeline.py
â”œâ”€â”€ constants.py
â””â”€â”€ utils.py
```

- Copy these files/folders from `scripts` into `Lead_scoring_data_pipeline`.

> ðŸ”¸ *Do not copy `utils_output.db`. A new DB will be created.*

---

#### **Step 15: Update Paths and DB Name**
After copying the files:

- Update the **file paths** in `constants.py`, `utils.py`, and `lead_scoring_data_pipeline.py`.
- Change the DB name to: `lead_scoring_data_cleaning.db` in `constants.py`.

---

#### **Step 16: Run the Pipeline via Airflow**
To configure and run the DAG:

**A. Modify `airflow.cfg`:**
- `base_url` â†’ `http://localhost:6007`
- `web_server_port` â†’ `6007`

**B. Run the following commands:**
1. Initialize the DB:
   ```bash
   airflow db init
   ```
2. Create a user:
   ```bash
   airflow users create \
     --username <your_username> \
     --firstname <your_firstname> \
     --lastname <your_lastname> \
     --role Admin \
     --email <your_email> \
     --password <your_password>
   ```

3. Start the webserver:
   ```bash
   airflow webserver
   ```

4. Open a new terminal and start the scheduler:
   ```bash
   airflow scheduler
   ```

**C. Access Airflow:**
- Open the Jarvis Dashboard.
- Select API Endpoint 1.
- Log in using the credentials from step B.

> ðŸ”¸ *Take a screenshot of the Airflow UI for submission.*